{
    "version": "https://jsonfeed.org/version/1",
    "title": "Sidey",
    "home_page_url": "http://0.0.0.0:8700/",
    "feed_url": "http://0.0.0.0:8700/feed.json",
    "description": "Simple and minimalistic jekyll blogging theme.",
    "icon": "http://0.0.0.0:8700/apple-touch-icon.png",
    "favicon": "http://0.0.0.0:8700/favicon.ico",
    "expired": false,
    
    "author": "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}",
    
"items": [
    
        {
            "id": "http://0.0.0.0:8700/2023/03/01/llm-based-sr-report-generating",
            "title": "LLM Based SR Report Generating",
            "summary": null,
            "content_text": "IntroductionRecently, the majority of companies have been publishing Sustainability Reports (SR). However, due to a lack of ESG (Environmental, Social, and Governance) expertise, many companies outsource the report writing to ESG consulting firms. From the perspective of these consulting firms, they often lack knowledge of the specialized areas of the commissioning companies.For this project, we conducted a survey among employees experienced in writing SR reports to identify the stages that require the most time and effort. The interviews consistently revealed the two major challenges:Extracting keywords through company research and using them to craft appropriate sentences.Studying the commissioning company’s expertise to create relevant content.Among these, drafting content based on extracted keywords was the most time-consuming and challenging aspect. Hence, this project aims to address these issues to improve work efficiency.ObjectiveConsidering the automation potential and work processes involved in report production, our project has the following three objectives:Report Writing Style AutomationSentence Generation based on KeywordsImage Generation related to KeywordsThese objectives were determined through surveys and interviews.Survey ResultsThe difficulty and specialization of an SR report can vary, but considering the criterion of one section and one image, it can be observed that a significant proportion of time is invested in the overall report writing process.&lt;/p&gt;        MethodologyIn the initial design phase, if a discrepancy of more than 1% occurs in the outputs, prompt designs are modified for effectiveness.However, if discrepancies are below 1%, unintended outputs are adjusted for consistency through post-processing at the code level.Fine-tuning for each sentence generation topic (e.g., precise and factual sentences for specialized terms rather than creative sentences).AchievementsApproximately 80% reduction in time taken for sentence and image composition.Average report writing time reduced from approximately 70 hours to less than 40 hours.Demo LinkDemo LinkFor the demo, we have utilized the SR report of Lego Chem Bio, adapted in a report writing style.The demo includes image generation in four different styles.",
            "content_html": "<h1 id=\"introduction\">Introduction</h1><hr /><p>Recently, the majority of companies have been publishing Sustainability Reports (SR). However, due to a lack of ESG (Environmental, Social, and Governance) expertise, many companies outsource the report writing to ESG consulting firms. From the perspective of these consulting firms, they often lack knowledge of the specialized areas of the commissioning companies.</p><p>For this project, we conducted a survey among employees experienced in writing SR reports to identify the stages that require the most time and effort. The interviews consistently revealed the two major challenges:</p><p>Extracting keywords through company research and using them to craft appropriate sentences.Studying the commissioning company’s expertise to create relevant content.Among these, drafting content based on extracted keywords was the most time-consuming and challenging aspect. Hence, this project aims to address these issues to improve work efficiency.<br /></p><h1 id=\"objective\">Objective</h1><hr /><p>Considering the automation potential and work processes involved in report production, our project has the following three objectives:</p><p>Report Writing Style AutomationSentence Generation based on KeywordsImage Generation related to KeywordsThese objectives were determined through surveys and interviews.<br /></p><h1 id=\"survey-results\">Survey Results</h1><hr /><p>The difficulty and specialization of an SR report can vary, but considering the criterion of one section and one image, it can be observed that a significant proportion of time is invested in the overall report writing process.&lt;/p&gt;</p><p align=\"center\">    <img width=\"800\" src=\"/assets/2023/llm/survey1.png\" /></p><p align=\"center\">    <img width=\"800\" src=\"/assets/2023/llm/survey1.png\" /></p><p><br /></p><h1 id=\"methodology\">Methodology</h1><hr /><p>In the initial design phase, if a discrepancy of more than 1% occurs in the outputs, prompt designs are modified for effectiveness.However, if discrepancies are below 1%, unintended outputs are adjusted for consistency through post-processing at the code level.Fine-tuning for each sentence generation topic (e.g., precise and factual sentences for specialized terms rather than creative sentences).AchievementsApproximately 80% reduction in time taken for sentence and image composition.Average report writing time reduced from approximately 70 hours to less than 40 hours.<br /></p><h1 id=\"demo-link\">Demo Link</h1><hr /><p><a href=\"http://report.jaewoo-so.online/\" style=\"color: blue; text-decoration: underline;\">Demo Link</a>For the demo, we have utilized the SR report of Lego Chem Bio, adapted in a report writing style.The demo includes image generation in four different styles.<br /></p>",
            "url": "http://0.0.0.0:8700/2023/03/01/llm-based-sr-report-generating",
            
            
            
            
            
            "date_published": "2023-03-01T00:00:00+09:00",
            "date_modified": "2023-03-01T00:00:00+09:00",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://0.0.0.0:8700/2023/01/01/esg-media-ml-service",
            "title": "ESG Media Machine-Learning Service Pipeline",
            "summary": null,
            "content_text": "Link  KESG Media Portal Site  Tutorial : Simple ML Pipeline with Kubernetes + Restful API Service Infra  Kubernetes : Microk8s ( 1 Master + 2 Worker)  Github Action  MicroService Architecture : Flask + Kubernetes  Front : Dash for ProtoType  DataBase : Postgres , mySQL  GPU : RTX 3090 x 2Model  python, dash  pytorch, transformer, tensorflow  BERT (use embedding layer that fine tuned with KLUE dataset) , LightGBM, Mecab, Konlpy, Scikit-learn,1. Why Kubernetes?  When there were many samples or intermittent network problems, people had to repair them after monitoring each time.Therefore, self-healing-enabled Kubernetes automates these monitoring and repairs to reduce labor costs.  Since large-scale traffic could occur in the future, the Load Balance function was required.  Monitoring functions such as grafana can be set conveniently.  Despite the resource limitations of single-person development, many useful functions can be easily implemented.2. Why MicroService Architecture?  Service extension is planned in the future, and it is desgined to reuse existing functions by separating them into functional units.3. Why Github Action?  Organized so that functions can be applied directly from the development server to the service server through Github Action with manifest4. System Design4. Real Service ScreenshotFront    ESG Issue Analysis     Target Company Monitoring      Target Company News List        Data Center  ",
            "content_html": "<p><strong>Link</strong></p><ul>  <li><a href=\"http://portal.kresg.co.kr\" style=\"color: blue; text-decoration: underline;\">KESG Media Portal Site</a></li>  <li><a href=\"\">Tutorial : Simple ML Pipeline with Kubernetes + Restful API </a></li></ul><p><strong>Service Infra</strong></p><ul>  <li>Kubernetes : Microk8s ( 1 Master + 2 Worker)</li>  <li>Github Action</li>  <li>MicroService Architecture : Flask + Kubernetes</li>  <li>Front : Dash for ProtoType</li>  <li>DataBase : Postgres , mySQL</li>  <li>GPU : RTX 3090 x 2</li></ul><p><strong>Model</strong></p><ul>  <li>python, dash</li>  <li>pytorch, transformer, tensorflow</li>  <li>BERT (use embedding layer that fine tuned with KLUE dataset) , LightGBM, Mecab, Konlpy, Scikit-learn,</li></ul><h2 id=\"1-why-kubernetes\">1. Why Kubernetes?</h2><ul>  <li>When there were many samples or intermittent network problems, people had to repair them after monitoring each time.Therefore, self-healing-enabled Kubernetes automates these monitoring and repairs to reduce labor costs.</li>  <li>Since large-scale traffic could occur in the future, the Load Balance function was required.</li>  <li>Monitoring functions such as grafana can be set conveniently.</li>  <li>Despite the resource limitations of single-person development, many useful functions can be easily implemented.</li></ul><h2 id=\"2-why-microservice-architecture\">2. Why MicroService Architecture?</h2><ul>  <li>Service extension is planned in the future, and it is desgined to reuse existing functions by separating them into functional units.</li></ul><h2 id=\"3-why-github-action\">3. Why Github Action?</h2><ul>  <li>Organized so that functions can be applied directly from the development server to the service server through Github Action with manifest</li></ul><h2 id=\"4-system-design\">4. System Design</h2><p><img src=\"/assets/esg_media/pipeline/kube_pipeline_trans.png\" alt=\"kubernetes_pipeline\" /></p><h2 id=\"4-real-service-screenshot\">4. Real Service Screenshot</h2><p><strong>Front</strong>  <br /><img src=\"/assets/esg_media/webpage/kresg_front.png\" alt=\"front\" />  <br /><br /><strong>ESG Issue Analysis</strong>  <br /><img src=\"/assets/esg_media/webpage/kresg_issue.png\" alt=\"issue_analysis\" />  <br /><br /> <br /><strong>Target Company Monitoring</strong>  <br /><img src=\"/assets/esg_media/webpage/kresg_monitoring.png\" alt=\"monitoring\" />  <br /><br />  <br /><strong>Target Company News List</strong>  <br /><img src=\"/assets/esg_media/webpage/kresg_news_list.png\" alt=\"news\" />     <br /><br /> <br /><strong>Data Center</strong>  <br /><img src=\"/assets/esg_media/webpage/kresg_datacenter.png\" alt=\"data_center\" /></p>",
            "url": "http://0.0.0.0:8700/2023/01/01/esg-media-ml-service",
            
            
            
            
            
            "date_published": "2023-01-01T00:00:00+09:00",
            "date_modified": "2023-01-01T00:00:00+09:00",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://0.0.0.0:8700/2022/06/01/clinical-decision-support-algorithm-to-anti-pd1-therapy",
            "title": "Clinical decision support algorithm based on machine learning to assess the clinical response to anti–pd-1 therapy",
            "summary": null,
            "content_text": "A tissue origin prediction device, method of predicting the tissue origin using a genome data, and computer program1020200076756 · Filed Jun 23, 2020SummaryAnti-programmed death (PD)-1 therapy (αPD-1) has been used in patients with non-small celllung cancer (NSCLC), leading to improved outcomes. However, the outcomes of therapy are stillinsufficient, and the expression of PD-ligand 1 (PD-L1) is not always a predictor of response toαPD-1.The immuno-cancer drugs used for treatment purposes of NSCLC are typically Kitruda and Optivo. Each monthly treatment costs about $600,000 and $800,000, respectively, and costs nearly $10 million a year.But the drug doesn’t respond to all patients. Therefore, in the case of Kitruda, the ‘PD-L1 expression positive rate’ must be 50% or more to be prescribed. However, even if the conditions are satisfied and prescribed, the actual drug reaction is about 60%.I studied a drug activity prediction model using Non-FDA-approved information to help more patients get the correct prescription and reduce the cost of drug waste.Summary Idea &amp; SolutionData  Clinical data including patient characteristics  Mutations  Laboratory findings from the electronic medical recordsData Feature  Missing values ​​exist in most features. Learning inputation models separately based on the distribution of other features and their non-missing values, rather than simple mean, zero, etc. methods.  To prevent data leakage, it was reviewed directly by the clinician.Model Selection &amp; Evaluation  Proceed with model selection, including linear models, to find the appropriate complexity of the model.  Due to the small number of samples, LOOCV is used for model selection  Due to the nature of the field, some explanatory power is required, so the ensemble stage to maximize performance is not performed.  Finding samples that appear to be somewhat outliers as a result of LOOCV. (4 samples)Feature Attribution  It is very likely that there is an interaction effect between data.  Therefore, feature importance based on simple entropy is highly likely to cause errors in interpretation.  The Lime-based explanation model has a small number of samples, so it is judged that there is a lot of room for problems in fitting the local model.  For each sample, the average of the shap values ​​and the interaction value were calculated.DetailFeature engineeringGene&amp; Metastasis FeatureSparsity When judging based on the commonly used sparsity judgment criteria, it was determined that there was sparsity by the ratio of zero value.  Percentage of zero values: About \\(Gene Expression  \\in \\{0,1\\}\\),  The ratio of zero is at least 85% to a maximum of 95%. Therefore, it is judged to be sparse.  Number of observations: It does not see the sparse by observices.  Data Volatility: Because it is a binary, the conclusion is the same as that of Percenrage of zero. it is sparse.  Model performance: Unable to judge due to the limit of the number of samples.  Domain knowledge: Since there are no accurate statistics on gene expression for Korean, sparsity cannot be determined based on Domain knowledge.New Feature  To reduce sparsity, change the gene feature to a new feature that represents the total number of expressions of the associated gene.\\(\\text{driver oncogene} = \\sum{ \\text{gene expression}}\\)  Change Metastasis to “Total Metastasis” in the same way.\\(\\text{metastasis count} = \\sum{ \\text{metastasis present}}\\)  Neutrophil and Lymphocyte were replaced by the LNR (lymphocyte-to-neutrophil ratio) feature.\\(LNR = log(\\frac{Neutrophil}{Lymphocyte} + \\epsilon )\\)Model SelectionBelow is a comparison table of the eight models of the roc-auc score. It was evaluated with LOOCV (Leave one out CV).It was judged that the model required for prediction did not have to be highly complex. Therefore, we did not do model ensembles and compared only single models.Best Model Performance : LightGBMFeature AttributionIt is the average value of the SHAP value of all samples.Shap is the contribution to the predicted value of a model considering the synergistic effect of one feature and another feature, from the point of view of game theory. Since the data in the medical and bio fields are features that are difficult to assert independence, it was judged that SHAP value-based interpretation was appropriate.Looking at the results, I suspected that the persistence of non-measurable lesions might have a high correlation with the target value. Thus, the persistence of non-measurable lesions is an ordinary type of data, so we look at spearman correlation.correlation = -0.44 and p-value = 8.1e-11. That is, it was not a strong relationship, so it was used as a feature.",
            "content_html": "<h2 id=\"a-tissue-origin-prediction-device-method-of-predicting-the-tissue-origin-using-a-genome-data-and-computer-program\">A tissue origin prediction device, method of predicting the tissue origin using a genome data, and computer program</h2><h3 id=\"1020200076756--filed-jun-23-2020\">1020200076756 · Filed Jun 23, 2020</h3><h1 id=\"summary\">Summary</h1><hr /><p>Anti-programmed death (PD)-1 therapy (αPD-1) has been used in patients with non-small celllung cancer (NSCLC), leading to improved outcomes. However, the outcomes of therapy are stillinsufficient, and the expression of PD-ligand 1 (PD-L1) is not always a predictor of response toαPD-1.</p><p>The immuno-cancer drugs used for treatment purposes of NSCLC are typically Kitruda and Optivo. <br />Each monthly treatment costs about $600,000 and $800,000, respectively, and costs nearly $10 million a year.</p><p>But the drug doesn’t respond to all patients. Therefore, in the case of Kitruda, the ‘PD-L1 expression positive rate’ must be 50% or more to be prescribed. However, even if the conditions are satisfied and prescribed, the actual drug reaction is about 60%.</p><p>I studied a drug activity prediction model using Non-FDA-approved information to help more patients get the correct prescription and reduce the cost of drug waste.</p><h1 id=\"summary-idea--solution\">Summary Idea &amp; Solution</h1><hr /><h2 id=\"data\">Data</h2><ul>  <li>Clinical data including patient characteristics</li>  <li>Mutations</li>  <li>Laboratory findings from the electronic medical records</li></ul><h2 id=\"data-feature\">Data Feature</h2><ul>  <li>Missing values ​​exist in most features. Learning inputation models separately based on the distribution of other features and their non-missing values, rather than simple mean, zero, etc. methods.</li>  <li>To prevent data leakage, it was reviewed directly by the clinician.</li></ul><h2 id=\"model-selection--evaluation\">Model Selection &amp; Evaluation</h2><ul>  <li>Proceed with model selection, including linear models, to find the appropriate complexity of the model.</li>  <li>Due to the small number of samples, LOOCV is used for model selection</li>  <li>Due to the nature of the field, some explanatory power is required, so the ensemble stage to maximize performance is not performed.</li>  <li>Finding samples that appear to be somewhat outliers as a result of LOOCV. (4 samples)</li></ul><h2 id=\"feature-attribution\">Feature Attribution</h2><ul>  <li>It is very likely that there is an interaction effect between data.</li>  <li>Therefore, feature importance based on simple entropy is highly likely to cause errors in interpretation.</li>  <li>The Lime-based explanation model has a small number of samples, so it is judged that there is a lot of room for problems in fitting the local model.</li>  <li>For each sample, the average of the shap values ​​and the interaction value were calculated.</li></ul><p><img src=\"/assets/paper_cdss/CDSS_main.jpg\" alt=\"main_fig\" /><img src=\"/assets/paper_cdss/paper_front.png\" alt=\"front\" /></p><p><br /></p><h1 id=\"detail\">Detail</h1><hr /><h2 id=\"feature-engineering\">Feature engineering</h2><h3 id=\"gene-metastasis-feature\">Gene&amp; Metastasis Feature</h3><p><strong>Sparsity</strong> <br /></p><p>When judging based on the commonly used sparsity judgment criteria, it was determined that there was sparsity by the ratio of zero value.</p><ul>  <li>Percentage of zero values: About \\(Gene Expression  \\in \\{0,1\\}\\),  The ratio of zero is at least 85% to a maximum of 95%. Therefore, it is judged to be sparse.</li>  <li>Number of observations: It does not see the sparse by observices.</li>  <li>Data Volatility: Because it is a binary, the conclusion is the same as that of Percenrage of zero. it is sparse.</li>  <li>Model performance: Unable to judge due to the limit of the number of samples.</li>  <li>Domain knowledge: Since there are no accurate statistics on gene expression for Korean, sparsity cannot be determined based on Domain knowledge.</li></ul><p><strong>New Feature</strong></p><ul>  <li>To reduce sparsity, change the gene feature to a new feature that represents the total number of expressions of the associated gene.\\(\\text{driver oncogene} = \\sum{ \\text{gene expression}}\\)</li>  <li>Change Metastasis to “Total Metastasis” in the same way.\\(\\text{metastasis count} = \\sum{ \\text{metastasis present}}\\)</li>  <li>Neutrophil and Lymphocyte were replaced by the LNR (lymphocyte-to-neutrophil ratio) feature.\\(LNR = log(\\frac{Neutrophil}{Lymphocyte} + \\epsilon )\\)</li></ul><p><br /></p><h2 id=\"model-selection\">Model Selection</h2><p>Below is a comparison table of the eight models of the roc-auc score. It was evaluated with LOOCV (Leave one out CV).It was judged that the model required for prediction did not have to be highly complex. Therefore, we did not do model ensembles and compared only single models.<br /><img src=\"/assets/paper_cdss/paper_compare.png\" alt=\"score\" /><br /></p><p><strong>Best Model Performance : LightGBM</strong></p><p><img src=\"/assets/paper_cdss/paper_score.png\" alt=\"score\" /></p><p><br /></p><h2 id=\"feature-attribution-1\">Feature Attribution</h2><hr /><p>It is the average value of the SHAP value of all samples.<br />Shap is the contribution to the predicted value of a model considering the synergistic effect of one feature and another feature, from the point of view of game theory. <br />Since the data in the medical and bio fields are features that are difficult to assert independence, it was judged that SHAP value-based interpretation was appropriate.</p><p><img src=\"/assets/paper_cdss/paper_shap_val.png\" alt=\"shap\" /></p><p>Looking at the results, I suspected that the persistence of non-measurable lesions might have a high correlation with the target value. Thus, the persistence of non-measurable lesions is an ordinary type of data, so we look at spearman correlation.correlation = -0.44 and p-value = 8.1e-11. That is, it was not a strong relationship, so it was used as a feature.</p>",
            "url": "http://0.0.0.0:8700/2022/06/01/clinical-decision-support-algorithm-to-anti-pd1-therapy",
            
            
            
            
            
            "date_published": "2022-06-01T00:00:00+09:00",
            "date_modified": "2022-06-01T00:00:00+09:00",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://0.0.0.0:8700/2021/04/01/esg-media-topic-classification-kr",
            "title": "ESG Media Topic Classification",
            "summary": null,
            "content_text": "Model is applied in below site for classification esg news issue and target company  KESG Media Portal Sitecontents  베이스 모델          클래스별 shap      특정 클래스 문제        모델 앙상블          모델 구성        모델 심화          클러스터링 : constrative 전후 비교      Part1 : Optimize topic class structure by ML model  문제          최종적으로 뉴스 분류 업무의 자동화가 목적이다.      의미론 관점에서의 분류 체계와 모델의 피쳐스페이스 관점에서의 분류 체계      기존에는 사람의 기준에서 뉴스 토픽의 기준을 만들었다.      하지만 피쳐 스페이스에서 정확히 분류되기 힘든 클래스들이 관측되었다.      이러한 클래스를 발견하고 클래스 분류 체계의 개선을 제안      2. 긍정/부정 모델긍부정 분류 모델의 경우 TF-IDF 기반의 피쳐, BERT 임베딩 벡터를 + Auto Encoder로 압축한 2000 차원의 피쳐를 Tree 계열 모델(Random Forest, LightGBM)에 실험했다. 피쳐를 BERT 임베딩 벡터를 사용한 경우가 성능이 더 좋았다.따라서 TF-IDF는 본 테스크에 적절한 패턴을 잡아내지 못한다고 판단했다. 결과는 아래와 같다.3. Base Model  먼저, Multiclass-classification 모델을 만들었다.  베이스모델은 TF-IDF 기반으로 만들었다. : (document frequency ignore 기준 Threshold 0.7 ~ 0.99 를 테스트, 대부분의 주요 단어가 누락되지 않는Threhold를 찾았다.)  적은 수의 클래스는 제외했다. 빨간 박스클래스 분포모델 성능Prediction Result Pattern베이스 모델의 결과 데이터의 클래스는 4가지로 특성이 관측되었다.  샘플 수 많음 , 정확도 높음  샘플 수 많음 , 정확도 낮음  샘플 수 적음 , 정확도 높음  샘플 수 적음 , 정확도 낮음베이스 모델의 precision/recall의 관찰로  샘플수에 비례하여 precision/recall가 높지 않은 메이저 클래스가 관찰된다. -&gt; 분석 필요  마이너 클래스 중에 특히나 precision/recall가 높은 클래스가 있다. -&gt; 피쳐 스페이스상 Small cluster preservation이 잘 되어 있는 클래스로 보인다. 4. Ensemble Model4.1 Class Imablance Measuremulti-majority multi-minority 확인을 위해서 imbalnace ratio, Imbalance-degree을 살펴보았다.  모델의 훈련 데이터 세트를 imbalnace의 정도에 따라서 컨트롤한다.먼저 multi-class 이기 때문에 imbalance ratio이 아닌 imbalance degree 측정해보았다. imbalance degree 측정을 위해 훈련 데이터의 class 비율을 class distribution이라고 가정했다. 그리고 distance metric으로 euclidian distance를 사용했다.Data info  k = 28 (num of class)  total sample size = about 100000Class imbalance degree            include ‘etc’ class      exclude ‘etc’ class                  22.64      18.36      명확한 imbalanced 문제이다. 그리고 클래스 샘플 수에 따라 multi-class classification problem \\(\\eta_{k}\\) 은 “ multi-minority imbalance”로 분류 할 수 있다.\\[\\eta_{k} \\text{ is multi-minority} \\iff \\sum^{K} \\mathbb{1}\\left(\\eta_{i} &lt; \\frac{1}{K} \\right) &gt; \\frac{K}{2}\\]4.2 Deal with imbalnceimbalance 문제를 다루기 위한 방법 중 Cost-sensitive Learning, Ensemble Methods를 사용하였다. Sampling 및 data Augmentation 기법을 사용하지 않은 이유는 다음과 같다.Why not use “Sampling” and “Data Augmentation” for ?  Oversampling, Undersampling : majority class에서는 심한 노이즈 또는 레이블링 오차등의 퀄리티 이슈가 보이는 클래스, minority class에서는 feature space 상 cluster preservation이 잘 되어있는 것으로 보이는 클래스 들이 존재한다. 따라서 샘플링 방식이 모든 클래스에 동등한 효과를 보이기를 기대하기 어렵다.  Data Augmentation :          ESG관련 내용에 대한 semantic meaning 연구가 없다.      GPT-3나 BERT같은 언어모델로 생성하는 방법이 있으나, ESG관련 부정적 기업의 이슈는 사실에 기반한 사건이므로 생성된 데이터의 오류가 클 가능성이 있다. 추가로 시간에 따른 concept shift가 관찰되기 때문에 오버피팅의 가능성이 크다.      4.2 Ensemble : Model splitmulti-minority5. Catergory modify  각 클래스의 샘플의 오분류에서 편향이 발생하는지 확인을 하였다.  LightGBM + Bert Embedding을 사용했고, class weight 사용 전후를 비교한 결과는 아래와 같다.샘플 수의 많고 적음은 500개를 기준으로 하였다. 샘플 수 많은데 정확도가 낮은 클래스를 샘플링 조사를 해보았다.데이터 자체에 topic이 여러개 인 경우가 많았다. 즉 사람도 정확히 맞추기 힘든 샘플들 이였다. 이는 모델이나 데이터로 해결할 수 없는 문제이기에 클래스 분류 체계를 변경하는 것을 제안했다.아래는 클래스의 분류 체계 변경 후 모델 성능이다.뉴스 재분류 작업은 많은 리소스 투입이 필요한 일이기에 현재의 훈련 데이터셋으로 서비스 가능 한 레벨의모델의 평가는 f-beta (beta = 0.5)로 설정 했다. 뉴스 데이터 특성상 중복 뉴스가 나오는 경우가 많아 recall 보다는 precision이 더욱 많도록",
            "content_html": "<p><strong>Model is applied in below site for classification esg news issue and target company</strong></p><ul>  <li><a href=\"http://portal.kresg.co.kr/\">KESG Media Portal Site</a></li></ul><h1 id=\"contents\">contents</h1><hr /><ol>  <li>베이스 모델    <ul>      <li>클래스별 shap</li>      <li>특정 클래스 문제</li>    </ul>  </li>  <li>모델 앙상블    <ul>      <li>모델 구성</li>    </ul>  </li>  <li>모델 심화    <ul>      <li>클러스터링 : constrative 전후 비교</li>    </ul>  </li></ol><h1 id=\"part1--optimize-topic-class-structure-by-ml-model\">Part1 : Optimize topic class structure by ML model</h1><hr /><ol>  <li>문제    <ul>      <li>최종적으로 뉴스 분류 업무의 자동화가 목적이다.</li>      <li>의미론 관점에서의 분류 체계와 모델의 피쳐스페이스 관점에서의 분류 체계</li>      <li>기존에는 사람의 기준에서 뉴스 토픽의 기준을 만들었다.</li>      <li>하지만 피쳐 스페이스에서 정확히 분류되기 힘든 클래스들이 관측되었다.</li>      <li>이러한 클래스를 발견하고 클래스 분류 체계의 개선을 제안<br /></li>    </ul>  </li></ol><h2 id=\"2-긍정부정-모델\">2. 긍정/부정 모델</h2><hr /><p>긍부정 분류 모델의 경우 TF-IDF 기반의 피쳐, BERT 임베딩 벡터를 + Auto Encoder로 압축한 2000 차원의 피쳐를 Tree 계열 모델(Random Forest, LightGBM)에 실험했다. 피쳐를 BERT 임베딩 벡터를 사용한 경우가 성능이 더 좋았다.따라서 TF-IDF는 본 테스크에 적절한 패턴을 잡아내지 못한다고 판단했다. 결과는 아래와 같다.</p><p><img src=\"/assets/esg_media/topic/pn_model_confusion.png\" alt=\"pn_table\" /><img src=\"/assets/esg_media/topic/pn_auc.png\" alt=\"pn_auc\" /><br /></p><h2 id=\"3-base-model\">3. Base Model</h2><hr /><ul>  <li>먼저, Multiclass-classification 모델을 만들었다.</li>  <li>베이스모델은 TF-IDF 기반으로 만들었다. : (document frequency ignore 기준 Threshold 0.7 ~ 0.99 를 테스트, 대부분의 주요 단어가 누락되지 않는Threhold를 찾았다.)</li>  <li>적은 수의 클래스는 제외했다. 빨간 박스</li></ul><p><br /></p><p><strong>클래스 분포</strong><img src=\"/assets/esg_media/topic/클래스_분포.png\" alt=\"class_dist\" /></p><p><br /></p><p><strong>모델 성능</strong><img src=\"/assets/esg_media/topic/전체모델성능.png\" alt=\"base_model_performance\" /></p><p><br /></p><h3 id=\"prediction-result-pattern\">Prediction Result Pattern</h3><p><br />베이스 모델의 결과 데이터의 클래스는 4가지로 특성이 관측되었다.</p><ul>  <li>샘플 수 많음 , 정확도 높음</li>  <li>샘플 수 많음 , 정확도 낮음</li>  <li>샘플 수 적음 , 정확도 높음</li>  <li>샘플 수 적음 , 정확도 낮음</li></ul><p>베이스 모델의 precision/recall의 관찰로</p><ol>  <li>샘플수에 비례하여 precision/recall가 높지 않은 메이저 클래스가 관찰된다. -&gt; 분석 필요</li>  <li>마이너 클래스 중에 특히나 precision/recall가 높은 클래스가 있다. -&gt; 피쳐 스페이스상 Small cluster preservation이 잘 되어 있는 클래스로 보인다. <br /></li></ol><h2 id=\"4-ensemble-model\">4. Ensemble Model</h2><hr /><h3 id=\"41-class-imablance-measure\">4.1 Class Imablance Measure</h3><p>multi-majority multi-minority 확인을 위해서 imbalnace ratio, Imbalance-degree을 살펴보았다.  모델의 훈련 데이터 세트를 imbalnace의 정도에 따라서 컨트롤한다.</p><p>먼저 multi-class 이기 때문에 imbalance ratio이 아닌 imbalance degree 측정해보았다. imbalance degree 측정을 위해 훈련 데이터의 class 비율을 class distribution이라고 가정했다. 그리고 distance metric으로 euclidian distance를 사용했다.</p><p><br /></p><h3 id=\"data-info\">Data info</h3><ul>  <li>k = 28 (num of class)</li>  <li>total sample size = about 100000</li></ul><p><br /></p><p><strong>Class imbalance degree</strong></p><table>  <thead>    <tr>      <th>include ‘etc’ class</th>      <th>exclude ‘etc’ class</th>    </tr>  </thead>  <tbody>    <tr>      <td>22.64</td>      <td>18.36</td>    </tr>  </tbody></table><p><br />명확한 imbalanced 문제이다. 그리고 클래스 샘플 수에 따라 multi-class classification problem \\(\\eta_{k}\\) 은 “ multi-minority imbalance”로 분류 할 수 있다.</p>\\[\\eta_{k} \\text{ is multi-minority} \\iff \\sum^{K} \\mathbb{1}\\left(\\eta_{i} &lt; \\frac{1}{K} \\right) &gt; \\frac{K}{2}\\]<h3 id=\"42-deal-with-imbalnce\">4.2 Deal with imbalnce</h3><p><br /></p><p>imbalance 문제를 다루기 위한 방법 중 Cost-sensitive Learning, Ensemble Methods를 사용하였다. Sampling 및 data Augmentation 기법을 사용하지 않은 이유는 다음과 같다.</p><p><br /></p><p><strong>Why not use “Sampling” and “Data Augmentation” for ?</strong></p><ul>  <li>Oversampling, Undersampling : majority class에서는 심한 노이즈 또는 레이블링 오차등의 퀄리티 이슈가 보이는 클래스, minority class에서는 feature space 상 cluster preservation이 잘 되어있는 것으로 보이는 클래스 들이 존재한다. 따라서 샘플링 방식이 모든 클래스에 동등한 효과를 보이기를 기대하기 어렵다.</li>  <li>Data Augmentation :    <ul>      <li>ESG관련 내용에 대한 semantic meaning 연구가 없다.</li>      <li>GPT-3나 BERT같은 언어모델로 생성하는 방법이 있으나, ESG관련 부정적 기업의 이슈는 사실에 기반한 사건이므로 생성된 데이터의 오류가 클 가능성이 있다. 추가로 시간에 따른 concept shift가 관찰되기 때문에 오버피팅의 가능성이 크다.</li>    </ul>  </li></ul><hr /><h3 id=\"42-ensemble--model-split\">4.2 Ensemble : Model split</h3><p>multi-minority</p><h2 id=\"5-catergory-modify\">5. Catergory modify</h2><ul>  <li>각 클래스의 샘플의 오분류에서 편향이 발생하는지 확인을 하였다.</li>  <li></li></ul><p>LightGBM + Bert Embedding을 사용했고, class weight 사용 전후를 비교한 결과는 아래와 같다.</p><p><img src=\"/assets/esg_media/topic/\" alt=\"\" /></p><hr /><p>샘플 수의 많고 적음은 500개를 기준으로 하였다. 샘플 수 많은데 정확도가 낮은 클래스를 샘플링 조사를 해보았다.</p><p>데이터 자체에 topic이 여러개 인 경우가 많았다. 즉 사람도 정확히 맞추기 힘든 샘플들 이였다. 이는 모델이나 데이터로 해결할 수 없는 문제이기에 클래스 분류 체계를 변경하는 것을 제안했다.</p><p>아래는 클래스의 분류 체계 변경 후 모델 성능이다.</p><p>뉴스 재분류 작업은 많은 리소스 투입이 필요한 일이기에 현재의 훈련 데이터셋으로 서비스 가능 한 레벨의</p><p>모델의 평가는 f-beta (beta = 0.5)로 설정 했다. 뉴스 데이터 특성상 중복 뉴스가 나오는 경우가 많아 recall 보다는 precision이 더욱 많도록</p>",
            "url": "http://0.0.0.0:8700/2021/04/01/esg-media-topic-classification-kr",
            
            
            
            
            
            "date_published": "2021-04-01T00:00:00+09:00",
            "date_modified": "2021-04-01T00:00:00+09:00",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://0.0.0.0:8700/2020/09/01/tax-optimized-asset-management",
            "title": "Tax Optimized Asset management",
            "summary": null,
            "content_text": "In progress",
            "content_html": "<h3 id=\"in-progress\">In progress</h3>",
            "url": "http://0.0.0.0:8700/2020/09/01/tax-optimized-asset-management",
            
            
            
            
            
            "date_published": "2020-09-01T00:00:00+09:00",
            "date_modified": "2020-09-01T00:00:00+09:00",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://0.0.0.0:8700/2020/06/01/ctd-squared-pancancer-chemosensitivity-dream-challenge",
            "title": "CTD-squared Pancancer Chemosensitivity DREAM Challenge",
            "summary": null,
            "content_text": "Gene Expression Searching and machine learning model for Chemosensitivity Predictioncontents  Data  Workflow &amp; Key ideas  Feature Engineering  Conclusion &amp; Discussion  Reference  If you want to see entire predictive system concept, see section 2  If you want to detail of machine learning strategies, see section 5      The data used in this work are all public data.    In the middle, we changed the working environment to the AWS environment. The code for this repository is still unorganized, so it can be messy. And because the data is very large, I didn’t put it in the repository.  1. DataUsed Dataset  CCLE basal expression &amp; meta info  L1000 phase1, phase2 lv.5  CTRP AUC  DEMETER2 normalized dependency score for 515 cell lines  PANACEA gene expressionPreprocessingUse the following five data to describe the characteristics of the cell line.Histology and basal expressions for 515 cell lines in CCLE ,TF activity inference score and Pathway inference score were calculated from the CCLE basal expression value using Viper. PROGENy, respectively.The NA values within the DEMETER2 score were imputed with average values per cell lines.For the data to describe the characteristics of the drug, only post-treatment expression values were used.Signatures for overlapping 326 drugs in CTRP and L1000 are selected and only 973 experimentally measured genes were used to normalize by MODZ.The given PANACEA expression values were also normalized by the MODZ method across the cell lines.\\[\\mathcal{D_f} = \\{  (x_i,y_i) \\vert  f \\in F\\}\\]where \\(y\\) is auc value of perturbation, \\(x_i\\) is feature of each sample.\\(\\mathcal{S}\\) is feature class, \\(\\mathcal{S} = \\{TF,PRO,HIST,GENE,CCLE,D2_{mean} \\}\\), \\(\\mathcal{F}\\) is  feature set, \\(\\mathcal{F} = \\{f_i \\vert  i \\in S \\}\\)\\[f^* = \\{ TF,HIST,GENE,CCLE,D2_{mean} \\}\\]  2. Workflow &amp; Key ideas Step1. Searching similar drugs for hidden drugsFirst, we performed the gene expression signature search via spearman correlation calculation. As a reference data, L1000, which was signaturized for each drug through MODZ, and PANACEA was used as a query for this. The similarity between drugs was calculated by spearman correlation. We chose similar drugs with the following criteria. By calculating robust z-score for the similarity matrix, only drugs corresponding to more than 70% of the max value of z-score were collected for each query drug. For example, for the above method, there are 7 and 19 drugs selected as drugs similar to cmpd_KW and cmpd_WW, respectively.     Step2. Modeling &amp; PredictionAs shown above, a drug-specific model was created using only the selected perturbations for each hidden drug.The model approached the regression problem using the GBDT to predict the AUC value according to the cell line-drug pairs given in the CTRP and used features describing the above mentioned. We use CART for base learner with histogram optimized approximate greedy algorithm.  We finally found out that roughly 3000 features exhibited the best performance, and we actually saw that performance was not significantly reduced even if we reduced to 7 feature sets through dimension reduction.  The most efficient dimensional reduction model was the Autoencoder model with MSE + Correlation*0.7 as the objective, but due to the complicated analysis, we use the gradient boosting method with the CART for base learner and use a very small number of sampled features. And by overfitting each prediction model to a specific set of drugs,  final ensemble model had better generalization performance since the diversity of models increased.     Step3. EnsembleFor generalization performance, we implemented a model ensemble. Using the results of varying the threshold value from 20% to 90% of max with z-score as shown above, correlation between the results of each model was compared. To use more diverse models, eight models were finally selected in the order in which they were less correlated with each other, and the predicted results were integrated to create a final presentation file.Detail procedure is like this.\\(\\mathcal{M} \\textrm{ is set of models,} \\mathcal{M} := \\{ M_1 , \\cdots , M_n \\}, \\mathcal{D} : \\mathcal{M} \\times  \\mathcal{M} \\rightarrow \\mathcal{R} \\textrm{ is distance function}\\) and we define average distance over each model.\\[\\mathcal{D}_{avg}(M_i) = \\frac{1}{| \\mathcal{M} \\setminus { \\{i\\} }  |}  \\sum_{m \\in \\mathcal{M} \\backslash {\\{i\\}} } \\mathcal{D}(M_i,m)\\]next, we select candidate for ensemble .\\[\\mathcal{M}_{candidate} :=\\{  \\mathcal{M}_i | \\mathcal{D}_{avg}(M_i) \\ngeq \\textrm{ 90th percentile} \\}\\]finaly, define of ensemble score is like below .\\[S_{ensemble} = exp( \\frac{1}{|\\mathcal{M}_{candidate}|}  \\sum_{m \\in \\mathcal{M}_{candidate}} log( \\mathcal{D}_{avg}(m)) )\\]3. Feature Engineering1. Data Integration   Different public datasets have different gene types. Alternatively, you can use a gene set that you deem valid based on domain knowledge. Strategies other than intersection result in missing value unconditionally. Therefore, the method of imputating the missing value must also be selected.      In this project, we compared two methods using gene sets judged to be valid through intersection and paper search, and found that the performance of intersection is similar. Therefore, we used an intersection geneset with a small data size. (973 genes.)2. Demesional reduction      Unable to find any studies for manifolds of data between total expression amount + chemical reaction amount. Therefore, Autoencoder was used.        Use cosine similarity and pearsons R as the evaluation metric. In a situation where the process of generating genetic data is not guaranteed to be the same, it is difficult to normalize when operating between different dataset. Therefore, Cosine similarity was used as the main and Pearson correlation coefficient was used as an adjunct.        VAE vs AE comparison. (see ‘/code_clean/st_04_mapping_drug_378norm_ctrp_auto_encoder/’)      Why perform demesional reduction and use cosine similarity for AE?  -&gt; I found that the genomic data we use is biased by sequencing machines and processes in training machine learning models. This was previously based on cancer tumor classification tests and genomic data.  These differences mainly occur in relative expression amounts, and it was experimentally found that the types of genes expressed are generally consistent.  This is why I used AE and cosine similarity as an evaluation metric.Why not UMAP(Uniform Manifold Approximation and Projection)? -&gt; In the case of UMAP, it is necessary to design a quality evaluation method of the pre-distance metric, search for the optimal metric, and optimize the projection parameters. And since the model description is not necessary for this competition, it was not used for the sake of time.Why not linear dimensionality reduction (like PCA, NMF)?   -&gt; Because the data is high-dimensional and sparse, the linear method does not fit.3. DNN encoder feature    -&gt; It is a method determined in the engineering process to create optimal features.  Experimentally tried several feature engineering and applied them because we achieved the best CV-score.3. Conclusion &amp; DiscussionWe started from the assumption that we could deduce the MoA of the drug via post-treatment gene expression on the drug.It is difficult to predict drug sensitivities that have specific targets and specific pathways(Koras, K et al. 2020). So, we recognized the need to make models for each drug, and we tried many things, such as looking at which gene has high coefficient for each drug by predicting AUC with only gene expression, in addition to the methods described above. Our experiments have also shown that it is better to make a model for each drugs than to use one model predicting the AUC for all drugs.Each cell line has its own unique characteristics, so it is very meaningful to use it as a feature that can explain it. Predicting the drug sensitivities for each cell line will be very useful in selecting cell lines in experimental screening and also valuable in the field of new drug development in reducing costs.Since we didn’t have enough time to find the optimal model, we had no choice but to use the naive experimental results, so I think we can improve the predictive performance through more accurate experiments.5. References  CTD-squared Pancancer Chemosensitivity DREAM Challenge (syn21763589)  CTD-squared BeatAML DREAM Challenge (syn20940518)  Rees, M., Seashore-Ludlow, B., Cheah, J., Adams, D., Price, E., Gill, S., Javaid, S., Coletti, M., Jones, V., Bodycombe, N., Soule, C., Alexander, B., Li, A., Montgomery, P., Kotz, J., Hon, C., Munoz, B., Liefeld, T., Dančík, V., Haber, D., Clish, C., Bittker, J., Palmer, M., Wagner, B., Clemons, P., Shamji, A., Schreiber, S. (2016). Correlating chemical sensitivity and basal gene expression reveals mechanism of action Nature Chemical Biology  12(2), 109-116. https://dx.doi.org/10.1038/nchembio.1986  Subramanian, A., Narayan, R., Corsello, S., Peck, D., Natoli, T., Lu, X., Gould, J., Davis, J., Tubelli, A., Asiedu, J., Lahr, D., Hirschman, J., Liu, Z., Donahue, M., Julian, B., Khan, M., Wadden, D., Smith, I., Lam, D., Liberzon, A., Toder, C., Bagul, M., Orzechowski, M., Enache, O., Piccioni, F., Johnson, S., Lyons, N., Berger, A., Shamji, A., Brooks, A., Vrcic, A., Flynn, C., Rosains, J., Takeda, D., Hu, R., Davison, D., Lamb, J., Ardlie, K., Hogstrom, L., Greenside, P., Gray, N., Clemons, P., Silver, S., Wu, X., Zhao, W., Read-Button, W., Wu, X., Haggarty, S., Ronco, L., Boehm, J., Schreiber, S., Doench, J., Bittker, J., Root, D., Wong, B., Golub, T. (2017). A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles Cell 171(6), 1437 1452.e17. https://dx.doi.org/10.1016/j.cell.2017.10.049  Szalai, B., Subramanian, V., Holland, C., Alföldi, R., Pusk, L., Saez-Rodriguez, J. (2019). Signatures of cell death and proliferation in perturbation transcriptomics data—from confounding factor to effective prediction Nucleic Acids Research  47(19), 10010-10026. https://dx.doi.org/10.1093/nar/gkz805  Koras, K., Juraeva, D., Kreis, J., Mazur, J., Staub, E., Szczurek, E. (2020). Feature selection strategies for drug sensitivity prediction Scientific Reports 10(1), 9377. https://dx.doi.org/10.1038/s41598-020-65927-9  Garcia-Alonso L, Holland C, Ibrahim M, Turei D, Saez-Rodriguez J (2019). “Benchmark and integration of resources for the estimation of human transcription factor activities.” Genome Research. doi: 10.1101/gr.240663.118.  Schubert M, Klinger B, Klünemann M, Sieber A, Uhlitz F, Sauer S, Garnett MJ, Blüthgen N, Saez-Rodriguez J. “Perturbation-response genes reveal signaling footprints in cancer gene expression.” Nature Communications: 10.1038/s41467-017-02391-6",
            "content_html": "<h1 id=\"gene-expression-searching-and-machine-learning-model-for-chemosensitivity-prediction\">Gene Expression Searching and machine learning model for Chemosensitivity Prediction</h1><hr /><p><strong><em>contents</em></strong></p><ol>  <li>Data</li>  <li>Workflow &amp; Key ideas</li>  <li>Feature Engineering</li>  <li>Conclusion &amp; Discussion</li>  <li>Reference</li></ol><ul>  <li>If you want to see entire predictive system concept, see section 2</li>  <li>If you want to detail of machine learning strategies, see section 5</li>  <li>    <p>The data used in this work are all public data.</p>  </li>  <li>In the middle, we changed the working environment to the AWS environment. The code for this repository is still unorganized, so it can be messy. And because the data is very large, I didn’t put it in the repository.</li></ul><p>  </p><h1 id=\"1-data\">1. Data</h1><h3 id=\"used-dataset\"><strong>Used Dataset</strong></h3><ul>  <li>CCLE basal expression &amp; meta info</li>  <li>L1000 phase1, phase2 lv.5</li>  <li>CTRP AUC</li>  <li>DEMETER2 normalized dependency score for 515 cell lines</li>  <li>PANACEA gene expression</li></ul><h3 id=\"preprocessing\"><strong>Preprocessing</strong></h3><p>Use the following five data to describe the characteristics of the cell line.Histology and basal expressions for 515 cell lines in CCLE ,TF activity inference score and Pathway inference score were calculated from the CCLE basal expression value using Viper. PROGENy, respectively.The NA values within the DEMETER2 score were imputed with average values per cell lines.</p><p>For the data to describe the characteristics of the drug, only post-treatment expression values were used.Signatures for overlapping 326 drugs in CTRP and L1000 are selected and only 973 experimentally measured genes were used to normalize by MODZ.The given PANACEA expression values were also normalized by the MODZ method across the cell lines.</p>\\[\\mathcal{D_f} = \\{  (x_i,y_i) \\vert  f \\in F\\}\\]<p>where \\(y\\) is auc value of perturbation, \\(x_i\\) is feature of each sample.</p><p>\\(\\mathcal{S}\\) is feature class, \\(\\mathcal{S} = \\{TF,PRO,HIST,GENE,CCLE,D2_{mean} \\}\\), \\(\\mathcal{F}\\) is  feature set, \\(\\mathcal{F} = \\{f_i \\vert  i \\in S \\}\\)</p>\\[f^* = \\{ TF,HIST,GENE,CCLE,D2_{mean} \\}\\]<p>  </p><h1 id=\"2-workflow--key-ideas\">2. Workflow &amp; Key ideas</h1><p> </p><h3 id=\"step1-searching-similar-drugs-for-hidden-drugs\">Step1. Searching similar drugs for hidden drugs</h3><hr /><p>First, we performed the gene expression signature search via spearman correlation calculation. As a reference data, L1000, which was signaturized for each drug through MODZ, and PANACEA was used as a query for this. The similarity between drugs was calculated by spearman correlation. We chose similar drugs with the following criteria. By calculating robust z-score for the similarity matrix, only drugs corresponding to more than 70% of the max value of z-score were collected for each query drug. For example, for the above method, there are 7 and 19 drugs selected as drugs similar to cmpd_KW and cmpd_WW, respectively.</p><p align=\"center\">    <img width=\"400\" src=\"/assets/2020/06_01/z_score_thereshold.png\" /></p><p> </p><h3 id=\"step2-modeling--prediction\">Step2. Modeling &amp; Prediction</h3><hr /><p>As shown above, a drug-specific model was created using only the selected perturbations for each hidden drug.The model approached the regression problem using the GBDT to predict the AUC value according to the cell line-drug pairs given in the CTRP and used features describing the above mentioned. We use CART for base learner with histogram optimized approximate greedy algorithm.  We finally found out that roughly 3000 features exhibited the best performance, and we actually saw that performance was not significantly reduced even if we reduced to 7 feature sets through dimension reduction.  The most efficient dimensional reduction model was the Autoencoder model with MSE + Correlation*0.7 as the objective, but due to the complicated analysis, we use the gradient boosting method with the CART for base learner and use a very small number of sampled features. And by overfitting each prediction model to a specific set of drugs,  final ensemble model had better generalization performance since the diversity of models increased.</p><p align=\"center\">    <img width=\"800\" src=\"/assets/2020/06_01/structure.png\" alt=\"Material Bread logo\" /></p><p> </p><h3 id=\"step3-ensemble\">Step3. Ensemble</h3><p>For generalization performance, we implemented a model ensemble. Using the results of varying the threshold value from 20% to 90% of max with z-score as shown above, correlation between the results of each model was compared. To use more diverse models, eight models were finally selected in the order in which they were less correlated with each other, and the predicted results were integrated to create a final presentation file.Detail procedure is like this.</p><p>\\(\\mathcal{M} \\textrm{ is set of models,} \\mathcal{M} := \\{ M_1 , \\cdots , M_n \\}, \\mathcal{D} : \\mathcal{M} \\times  \\mathcal{M} \\rightarrow \\mathcal{R} \\textrm{ is distance function}\\) and we define average distance over each model.</p>\\[\\mathcal{D}_{avg}(M_i) = \\frac{1}{| \\mathcal{M} \\setminus { \\{i\\} }  |}  \\sum_{m \\in \\mathcal{M} \\backslash {\\{i\\}} } \\mathcal{D}(M_i,m)\\]<p>next, we select candidate for ensemble .</p>\\[\\mathcal{M}_{candidate} :=\\{  \\mathcal{M}_i | \\mathcal{D}_{avg}(M_i) \\ngeq \\textrm{ 90th percentile} \\}\\]<p>finaly, define of ensemble score is like below .</p>\\[S_{ensemble} = exp( \\frac{1}{|\\mathcal{M}_{candidate}|}  \\sum_{m \\in \\mathcal{M}_{candidate}} log( \\mathcal{D}_{avg}(m)) )\\]<p><br /><br /></p><h1 id=\"3-feature-engineering\">3. Feature Engineering</h1><p><strong>1. Data Integration</strong>   <br />Different public datasets have different gene types. Alternatively, you can use a gene set that you deem valid based on domain knowledge. Strategies other than intersection result in missing value unconditionally. Therefore, the method of imputating the missing value must also be selected.     <br /> In this project, we compared two methods using gene sets judged to be valid through intersection and paper search, and found that the performance of intersection is similar. Therefore, we used an intersection geneset with a small data size. (973 genes.)</p><p><strong>2. Demesional reduction</strong></p><ul>  <li>    <p>Unable to find any studies for manifolds of data between total expression amount + chemical reaction amount. Therefore, Autoencoder was used.</p>  </li>  <li>    <p>Use cosine similarity and pearsons R as the evaluation metric. In a situation where the process of generating genetic data is not guaranteed to be the same, it is difficult to normalize when operating between different dataset. Therefore, Cosine similarity was used as the main and Pearson correlation coefficient was used as an adjunct.</p>  </li>  <li>    <p>VAE vs AE comparison. (see ‘/code_clean/st_04_mapping_drug_378norm_ctrp_auto_encoder/’)</p>  </li></ul><p align=\"center\">    <img width=\"600\" src=\"/assets/2020/06_01/vae_ae.png\" alt=\"Material Bread logo\" /></p><p><br /></p><p><strong>Why perform demesional reduction and use cosine similarity for AE?</strong>  <br />-&gt; I found that the genomic data we use is biased by sequencing machines and processes in training machine learning models. This was previously based on cancer tumor classification tests and genomic data.  <br /><br />These differences mainly occur in relative expression amounts, and it was experimentally found that the types of genes expressed are generally consistent.  <br /><br />This is why I used AE and cosine similarity as an evaluation metric.</p><p><br /></p><p><strong>Why not UMAP(Uniform Manifold Approximation and Projection)?</strong><br /> -&gt; In the case of UMAP, it is necessary to design a quality evaluation method of the pre-distance metric, search for the optimal metric, and optimize the projection parameters. And since the model description is not necessary for this competition, it was not used for the sake of time.</p><p><br /></p><p><strong>Why not linear dimensionality reduction (like PCA, NMF)?</strong>  <br /> -&gt; Because the data is high-dimensional and sparse, the linear method does not fit.</p><p><br /></p><p><strong>3. DNN encoder feature</strong>    <br />-&gt; It is a method determined in the engineering process to create optimal features.  <br /><br />Experimentally tried several feature engineering and applied them because we achieved the best CV-score.</p><p><br /><br /></p><h1 id=\"3-conclusion--discussion\">3. Conclusion &amp; Discussion</h1><p>We started from the assumption that we could deduce the MoA of the drug via post-treatment gene expression on the drug.It is difficult to predict drug sensitivities that have specific targets and specific pathways(Koras, K et al. 2020). So, we recognized the need to make models for each drug, and we tried many things, such as looking at which gene has high coefficient for each drug by predicting AUC with only gene expression, in addition to the methods described above. Our experiments have also shown that it is better to make a model for each drugs than to use one model predicting the AUC for all drugs.Each cell line has its own unique characteristics, so it is very meaningful to use it as a feature that can explain it. Predicting the drug sensitivities for each cell line will be very useful in selecting cell lines in experimental screening and also valuable in the field of new drug development in reducing costs.Since we didn’t have enough time to find the optimal model, we had no choice but to use the naive experimental results, so I think we can improve the predictive performance through more accurate experiments.</p><p><br /><br /></p><h1 id=\"5-references\">5. References</h1><ol>  <li>CTD-squared Pancancer Chemosensitivity DREAM Challenge (syn21763589)</li>  <li>CTD-squared BeatAML DREAM Challenge (syn20940518)</li>  <li>Rees, M., Seashore-Ludlow, B., Cheah, J., Adams, D., Price, E., Gill, S., Javaid, S., Coletti, M., Jones, V., Bodycombe, N., Soule, C., Alexander, B., Li, A., Montgomery, P., Kotz, J., Hon, C., Munoz, B., Liefeld, T., Dančík, V., Haber, D., Clish, C., Bittker, J., Palmer, M., Wagner, B., Clemons, P., Shamji, A., Schreiber, S. (2016). Correlating chemical sensitivity and basal gene expression reveals mechanism of action Nature Chemical Biology  12(2), 109-116. https://dx.doi.org/10.1038/nchembio.1986</li>  <li>Subramanian, A., Narayan, R., Corsello, S., Peck, D., Natoli, T., Lu, X., Gould, J., Davis, J., Tubelli, A., Asiedu, J., Lahr, D., Hirschman, J., Liu, Z., Donahue, M., Julian, B., Khan, M., Wadden, D., Smith, I., Lam, D., Liberzon, A., Toder, C., Bagul, M., Orzechowski, M., Enache, O., Piccioni, F., Johnson, S., Lyons, N., Berger, A., Shamji, A., Brooks, A., Vrcic, A., Flynn, C., Rosains, J., Takeda, D., Hu, R., Davison, D., Lamb, J., Ardlie, K., Hogstrom, L., Greenside, P., Gray, N., Clemons, P., Silver, S., Wu, X., Zhao, W., Read-Button, W., Wu, X., Haggarty, S., Ronco, L., Boehm, J., Schreiber, S., Doench, J., Bittker, J., Root, D., Wong, B., Golub, T. (2017). A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles Cell 171(6), 1437 1452.e17. https://dx.doi.org/10.1016/j.cell.2017.10.049</li>  <li>Szalai, B., Subramanian, V., Holland, C., Alföldi, R., Pusk, L., Saez-Rodriguez, J. (2019). Signatures of cell death and proliferation in perturbation transcriptomics data—from confounding factor to effective prediction Nucleic Acids Research  47(19), 10010-10026. https://dx.doi.org/10.1093/nar/gkz805</li>  <li>Koras, K., Juraeva, D., Kreis, J., Mazur, J., Staub, E., Szczurek, E. (2020). Feature selection strategies for drug sensitivity prediction Scientific Reports 10(1), 9377. https://dx.doi.org/10.1038/s41598-020-65927-9</li>  <li>Garcia-Alonso L, Holland C, Ibrahim M, Turei D, Saez-Rodriguez J (2019). “Benchmark and integration of resources for the estimation of human transcription factor activities.” Genome Research. doi: 10.1101/gr.240663.118.</li>  <li>Schubert M, Klinger B, Klünemann M, Sieber A, Uhlitz F, Sauer S, Garnett MJ, Blüthgen N, Saez-Rodriguez J. “Perturbation-response genes reveal signaling footprints in cancer gene expression.” Nature Communications: 10.1038/s41467-017-02391-6</li></ol>",
            "url": "http://0.0.0.0:8700/2020/06/01/ctd-squared-pancancer-chemosensitivity-dream-challenge",
            
            
            
            
            
            "date_published": "2020-06-01T00:00:00+09:00",
            "date_modified": "2020-06-01T00:00:00+09:00",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://0.0.0.0:8700/2019/12/24/methods-for-providing-information-about-responses-to-cancer-immunotherapy-and-devices-using-the-same",
            "title": "Methods for providing information about responses to cancer immunotherapy and devices using the same",
            "summary": null,
            "content_text": "Info  10-2021-0081547  Filed Dec 24, 2019Please refer to the following linkPostPaperPatent Cover",
            "content_html": "<h2 id=\"info\">Info</h2><ul>  <li>10-2021-0081547</li>  <li>Filed Dec 24, 2019</li></ul><p><br /></p><p><strong><em>Please refer to the following link</em></strong><a href=\"/__posts/2022-6-01-Clinical decision support algorithm to anti–pd-1 therapy.md\">Post</a><br /><a href=\"https://www.ejcancer.com/article/S0959-8049(21)00328-2/fulltext#%20\">Paper</a></p><p><br /></p><h3 id=\"patent-cover\">Patent Cover</h3><p><img src=\"/assets/patent_pd/patent_pdl1_front.png\" alt=\"score\" /></p>",
            "url": "http://0.0.0.0:8700/2019/12/24/methods-for-providing-information-about-responses-to-cancer-immunotherapy-and-devices-using-the-same",
            
            
            
            
            
            "date_published": "2019-12-24T00:00:00+09:00",
            "date_modified": "2019-12-24T00:00:00+09:00",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://0.0.0.0:8700/2019/12/01/gimhae-fire-prediction-competition",
            "title": "Gimhae Fire Prediction Competition",
            "summary": null,
            "content_text": "please go to below link for code and detail explain.Code and data available.go to repository01 EDA and Basic Feature engineeringJupyter notebook : 01 EDA and Basic Feature engineering  Basic feature preprocessing using EDA and domain knowledgeFirst, you need to check whether the data treated as NaN are missing or sparse.Then, the type of data is checked, and pre-processing and feature engineering are performed accordingly.Find answers to the questions below here.    A. Missing or Sparse?  B. What type of Missing or Sparse?  C. If values type is missing, is there any meaning of missing?02 Experimental Feature engineeringJupyter notebook : 02 Experimental Feature engineering  Feature engineering based on Cross Validation score and Test score.Base on Cross Validation Score01 convert missing value to min value : 강수량 prcpttn ()  In the feature of precipitation, NaN is judged to be unmeasurable because there is no precipitation. change to minimum value 002 Fill features of categorical and binary type with the most frequent values  Due to the time relationship, the imputation model is not trained separately, and is processed as the most frequent value03 Numeric type feature missing value handling  Processing based on domain knowledgeto zero  - ttl_grnd_flr : Sum of above-ground floors of buildings        - ttl_dwn_fr : Sum of basement floors of buildings     fill with mean value  Since the following features have a low null_ratio, better results can be obtained by imputation by creating k-mean or a separate prediction model.  As a matter of time, after imputation as an average, trials of other methods were followed.  - tmprtr : temperature (c)      - wnd_spd : wind speed      - wnd_drctn : wind direction      - hmdt : humidity     - hm_cnt : Administrative district Population     - bldng_ar : Building area      - fr_mn_cnt : Personnel of the fire department in charge    04 Target encoding with smoothing  Experimentation with valid encoding methods  1. target encoding  2. target encoding + smoothing  3. target encoding + noise (0.1)  4. target encoding + noise (0.4)  5. target encoding + smoothing + noise (0.1)  6. target encoding + smoothing + noise (0.4)05 Electrocity and Gas usage  When looking at the data distribution, there are samples with no usage.  Assuming that this would be an empty house or an unmanaged building, I created the feature as shown below.  In the case of a building without electricity, it is assumed that the risk of fire will be high due to lack of management.  Fire has nothing to do with usage, so it is expressed in binaryele_engry_us : Electricity usage for a specific period    gas_engry_us : Gas usage for a specific period    1 -&gt; A house that is being used or inhabited    0 -&gt; A building that is obsolete.    In 01_EDA_Missing_Value.ipynb, check that min value = 0 of electricity and gas consumption data.fr_mn_cnt: Number of fire department personnelThe number of fire department personnel in the jurisdiction is limited to a set  Assume that fire department personnel are assigned according to the size of the area and the frequency of fire occurrence.Set Binary to 1 if more than 100, 0 if less than 100    Based on test scoreMultiple encoding testsJupyter notebook : Experiment result LightgbmJupyter notebook : Experiment result Xgboost Feature engineeringwnd_drctn (0.08 score up)    Assuming that the wind direction is different for each season, there is a possibility of fire depending on the direction of the season  Data categorization into 4 types according to wind direction  Apply binning to wind direction (wnd_drctn)bin_hour : created feature (0.03 score up)    Assume that it will be difficult to report a fire at dawn.  As a result of the check, according to the statistics of Seoul, there is a difference in the occurrence of fires according to time zones.  Similarly, Gimhae Fire &amp; Marine Insurance determined that there was a difference in the frequency of fire occurrence by time period, and categorized data by time period.03 Data distribution control  Check the distribution of Train / Validation / Test data. Control the distribution similarly to the actual test data to be used.      What is unusual about this competition is that validation data was given. Therefore, it was assumed that at least the validation data would have a similar distribution to the test data, but the difference and correlation between the CV score and the LB score could not be derived, so the work was performed assuming that the distribution would be different.    Adverserial validation 및 UMAP dimension reduction visualization 으로 확인Jupyter notebook : Adverserial ValidationJupyter notebook : UMAP dimension reduction visualization04 Sampling  The following sampling strategies were used to control for differences in data distribution.1. A dimensionally reduced, low-dimensional vector that samples only samples that are close to the test sample.  euclidean distanceThe black dot is the test data. Model training proceeds by removing samples that are far from the black point distance and sampling only samples that are close to the black point.The criterion of closeness was experimentally determined.2. Picking with Predicted Probabilities  Create a train or not, validation or not, test or not classification model and extract train and validation samples that are indistinguishable from test.  Probability distribution of predicted values for test data or not.  Even though it is test data, there are also samples that are not test data. At this time, select training and validation samples that are predicted with a similar probability to the test data. (mainly between 0.05 and 0.15)  Conversely, samples predicted as test data are also selected even though they are not test data. (mainly between 0.8 and 1.0)",
            "content_html": "<p>please go to below link for code and detail explain.Code and data available.<a href=\"[/assets/2020/01_23/canberra.png](https://github.com/jaewoo-so/gimhae_fire_prediction)\">go to repository</a></p><h1 id=\"01-eda-and-basic-feature-engineering\">01 EDA and Basic Feature engineering</h1><hr /><p><a href=\"https://github.com/jaewoo-so/gimhae_fire_prediction/blob/master/code_final/01_EDA_Missing_Value_Basic_Processing.ipynb\" style=\"color: blue; text-decoration: underline;\">Jupyter notebook : 01 EDA and Basic Feature engineering</a></p><ul>  <li>Basic feature preprocessing using EDA and domain knowledge</li></ul><p>First, you need to check whether the data treated as NaN are missing or sparse.Then, the type of data is checked, and pre-processing and feature engineering are performed accordingly.</p><p>Find answers to the questions below here.  <br />  <br />A. Missing or Sparse?  <br />B. What type of Missing or Sparse?  <br />C. If values type is missing, is there any meaning of missing?</p><h1 id=\"02-experimental-feature-engineering\">02 Experimental Feature engineering</h1><hr /><p><a href=\"https://github.com/jaewoo-so/gimhae_fire_prediction/blob/master/code_final/01_EDA_Missing_Value_Basic_Processing.ipynb\" style=\"color: blue; text-decoration: underline;\">Jupyter notebook : 02 Experimental Feature engineering</a></p><ul>  <li>Feature engineering based on Cross Validation score and Test score.</li></ul><h2 id=\"base-on-cross-validation-score\">Base on Cross Validation Score</h2><h3 id=\"01-convert-missing-value-to-min-value--강수량-prcpttn-\">01 convert missing value to min value : 강수량 prcpttn ()</h3><ul>  <li>In the feature of precipitation, NaN is judged to be unmeasurable because there is no precipitation. change to minimum value 0</li></ul><h3 id=\"02-fill-features-of-categorical-and-binary-type-with-the-most-frequent-values\">02 Fill features of categorical and binary type with the most frequent values</h3><ul>  <li>Due to the time relationship, the imputation model is not trained separately, and is processed as the most frequent value</li></ul><h3 id=\"03-numeric-type-feature-missing-value-handling\">03 Numeric type feature missing value handling</h3><ul>  <li>Processing based on domain knowledge</li></ul><p><strong>to zero</strong></p><pre><code>  - ttl_grnd_flr : Sum of above-ground floors of buildings        - ttl_dwn_fr : Sum of basement floors of buildings     </code></pre><p><strong>fill with mean value</strong>  <br />Since the following features have a low null_ratio, better results can be obtained by imputation by creating k-mean or a separate prediction model.  <br />As a matter of time, after imputation as an average, trials of other methods were followed.</p><pre><code>  - tmprtr : temperature (c)      - wnd_spd : wind speed      - wnd_drctn : wind direction      - hmdt : humidity     - hm_cnt : Administrative district Population     - bldng_ar : Building area      - fr_mn_cnt : Personnel of the fire department in charge    </code></pre><h3 id=\"04-target-encoding-with-smoothing\">04 Target encoding with smoothing</h3><ul>  <li>Experimentation with valid encoding methods</li></ul><pre><code>  1. target encoding  2. target encoding + smoothing  3. target encoding + noise (0.1)  4. target encoding + noise (0.4)  5. target encoding + smoothing + noise (0.1)  6. target encoding + smoothing + noise (0.4)</code></pre><h3 id=\"05-electrocity-and-gas-usage\">05 Electrocity and Gas usage</h3><ul>  <li>When looking at the data distribution, there are samples with no usage.</li>  <li>Assuming that this would be an empty house or an unmanaged building, I created the feature as shown below.</li>  <li>In the case of a building without electricity, it is assumed that the risk of fire will be high due to lack of management.</li>  <li>Fire has nothing to do with usage, so it is expressed in binary</li></ul><pre><code>ele_engry_us : Electricity usage for a specific period    gas_engry_us : Gas usage for a specific period    1 -&gt; A house that is being used or inhabited    0 -&gt; A building that is obsolete.    </code></pre><p>In 01_EDA_Missing_Value.ipynb, check that min value = 0 of electricity and gas consumption data.</p><h3 id=\"fr_mn_cnt-number-of-fire-department-personnel\">fr_mn_cnt: Number of fire department personnel</h3><p>The number of fire department personnel in the jurisdiction is limited to a set  <br />Assume that fire department personnel are assigned according to the size of the area and the frequency of fire occurrence.</p><pre><code>Set Binary to 1 if more than 100, 0 if less than 100    </code></pre><h2 id=\"based-on-test-score\">Based on test score</h2><h3 id=\"multiple-encoding-tests\">Multiple encoding tests</h3><p><a href=\"https://github.com/jaewoo-so/gimhae_fire_prediction/blob/master/code_exp/sojaewoo/data_1111_version/code_v07/001_1_encoding_evaluation_lgb.ipynb\" style=\"color: blue; text-decoration: underline;\">Jupyter notebook : Experiment result Lightgbm</a></p><p><a href=\"https://github.com/jaewoo-so/gimhae_fire_prediction/blob/master/code_exp/sojaewoo/data_1111_version/code_v07/001_1_encoding_evaluation_xgb.ipynb\" style=\"color: blue; text-decoration: underline;\">Jupyter notebook : Experiment result Xgboost Feature engineering</a></p><h3 id=\"wnd_drctn-008-score-up\">wnd_drctn (0.08 score up)</h3><p align=\"center\">  <img src=\"/assets/2019/12_01/wind.png\" /></p><ul>  <li>Assuming that the wind direction is different for each season, there is a possibility of fire depending on the direction of the season</li>  <li>Data categorization into 4 types according to wind direction</li>  <li>Apply binning to wind direction (wnd_drctn)</li></ul><h2 id=\"bin_hour--created-feature-003-score-up\">bin_hour : created feature (0.03 score up)</h2><p align=\"center\">  <img src=\"/assets/2019/12_01/fire.png\" /></p><ul>  <li>Assume that it will be difficult to report a fire at dawn.</li>  <li>As a result of the check, according to the statistics of Seoul, there is a difference in the occurrence of fires according to time zones.</li>  <li>Similarly, Gimhae Fire &amp; Marine Insurance determined that there was a difference in the frequency of fire occurrence by time period, and categorized data by time period.</li></ul><h1 id=\"03-data-distribution-control\">03 Data distribution control</h1><hr /><ul>  <li>Check the distribution of Train / Validation / Test data. Control the distribution similarly to the actual test data to be used.</li>  <li>    <p>What is unusual about this competition is that validation data was given. Therefore, it was assumed that at least the validation data would have a similar distribution to the test data, but the difference and correlation between the CV score and the LB score could not be derived, so the work was performed assuming that the distribution would be different.</p>  </li>  <li>Adverserial validation 및 UMAP dimension reduction visualization 으로 확인</li></ul><p><a href=\"https://github.com/jaewoo-so/gimhae_fire_prediction/blob/master/code_final/etc_experiment/adverserial_validation/#01_adverserial_test_v11.ipynb\" style=\"color: blue; text-decoration: underline;\">Jupyter notebook : Adverserial Validation</a></p><p><a href=\"https://github.com/jaewoo-so/gimhae_fire_prediction/blob/master/code_final/etc_experiment/umap_dimentional_reduction/01_datav04_Apply_umap_test.ipynb\" style=\"color: blue; text-decoration: underline;\">Jupyter notebook : UMAP dimension reduction visualization</a></p><h1 id=\"04-sampling\">04 Sampling</h1><hr /><ul>  <li>The following sampling strategies were used to control for differences in data distribution.</li></ul><h2 id=\"1-a-dimensionally-reduced-low-dimensional-vector-that-samples-only-samples-that-are-close-to-the-test-sample\">1. A dimensionally reduced, low-dimensional vector that samples only samples that are close to the test sample.</h2><p align=\"center\">  <img src=\"/assets/2019/12_01/euclidian.png\" /></p><p>euclidean distance</p><p>The black dot is the test data. Model training proceeds by removing samples that are far from the black point distance and sampling only samples that are close to the black point.The criterion of closeness was experimentally determined.</p><h2 id=\"2-picking-with-predicted-probabilities\">2. Picking with Predicted Probabilities</h2><ul>  <li>Create a train or not, validation or not, test or not classification model and extract train and validation samples that are indistinguishable from test.</li></ul><p align=\"center\">  <img src=\"/assets/2019/12_01/te.png\" /></p><p>Probability distribution of predicted values for test data or not.  <br />Even though it is test data, there are also samples that are not test data. At this time, select training and validation samples that are predicted with a similar probability to the test data. (mainly between 0.05 and 0.15)  <br />Conversely, samples predicted as test data are also selected even though they are not test data. (mainly between 0.8 and 1.0)</p>",
            "url": "http://0.0.0.0:8700/2019/12/01/gimhae-fire-prediction-competition",
            
            
            
            
            
            "date_published": "2019-12-01T00:00:00+09:00",
            "date_modified": "2019-12-01T00:00:00+09:00",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        },
    
        {
            "id": "http://0.0.0.0:8700/2017/10/01/si-ge-thickness-and-composition-predict-system",
            "title": "Si-Ge Thickness and Composition Predict System",
            "summary": null,
            "content_text": "SummaryDevelopment of an epi-wafer thickness and composition ratio prediction model using Photoluminescence (PL) data. This research aims to build a machine learning model based on measured Photoluminescence phenomenon to minimize performance degradation and achieve cost reduction.What is Photoluminescence? Photoluminescence is a phenomenon observed in semiconductor wafers where they absorb light energy when exposed to laser irradiation and subsequently emit light. This principle is utilized in defect inspection by analyzing changes in light emission patterns caused by defects on wafers.BackgroundThe objective of this research is to maintain performance compared to a competitor’s product (KLA Tencor) while achieving cost reduction. Analysis indicates that the highest cost component in the competitor’s product is the nano-scale precision measurement with over 80 measurement sensors. Achieving nano-scale precision requires precision motors and vacuum stages, which are at least eight times more expensive than micro-meter motor-based products.SolutionPL Data Characteristics  Epi-wafer thickness and composition are continuous variables.  Light emission quantities at different depths are measured based on wavelength, resulting in three-dimensional continuous data.Methodology  Development of a machine learning model that predicts the value of a target point using surrounding point data.  Change the measurement method to perform partial mapping only for points required for prediction, thereby reducing inspection time.Feature Selection  Select the base model and conduct feature selection.  The objective of feature selection is as follows:\\[Objective_{feature} =  Correlation + \\alpha (\\text{num_point} \\times \\text{sensor_price})\\]Featrue Engineering  Since there are no outliers, only scaling is performed.Edge Data Processing  First, the predicted values are obtained using data up to just before the edge. Then, a spline fitting is applied to predict and generate edge data.Model SelectionExperimental results (model and feature-wise performance comparison)        Achievements  Cost efficiency achieved at 32% compared to competitor’s product.  Inspection speed achieved at 3 times faster compared to competitor’s product.  Performance reached 87% of competitor’s product.System Architecture    Final Result        ",
            "content_html": "<h1 id=\"summary\">Summary</h1><hr /><p>Development of an epi-wafer thickness and composition ratio prediction model using Photoluminescence (PL) data. This research aims to build a machine learning model based on measured Photoluminescence phenomenon to minimize performance degradation and achieve cost reduction.</p><p><strong>What is Photoluminescence?</strong> Photoluminescence is a phenomenon observed in semiconductor wafers where they absorb light energy when exposed to laser irradiation and subsequently emit light. This principle is utilized in defect inspection by analyzing changes in light emission patterns caused by defects on wafers.</p><p><br /></p><h1 id=\"background\">Background</h1><hr /><p>The objective of this research is to maintain performance compared to a competitor’s product (KLA Tencor) while achieving cost reduction. Analysis indicates that the highest cost component in the competitor’s product is the nano-scale precision measurement with over 80 measurement sensors. Achieving nano-scale precision requires precision motors and vacuum stages, which are at least eight times more expensive than micro-meter motor-based products.</p><p><br /></p><h1 id=\"solution\">Solution</h1><hr /><h2 id=\"pl-data-characteristics\">PL Data Characteristics</h2><ul>  <li>Epi-wafer thickness and composition are continuous variables.</li>  <li>Light emission quantities at different depths are measured based on wavelength, resulting in three-dimensional continuous data.</li></ul><h2 id=\"methodology\">Methodology</h2><ul>  <li>Development of a machine learning model that predicts the value of a target point using surrounding point data.</li>  <li>Change the measurement method to perform partial mapping only for points required for prediction, thereby reducing inspection time.</li></ul><h2 id=\"feature-selection\">Feature Selection</h2><ul>  <li>Select the base model and conduct feature selection.</li>  <li>The objective of feature selection is as follows:</li></ul>\\[Objective_{feature} =  Correlation + \\alpha (\\text{num_point} \\times \\text{sensor_price})\\]<h2 id=\"featrue-engineering\">Featrue Engineering</h2><ul>  <li>Since there are no outliers, only scaling is performed.</li></ul><h2 id=\"edge-data-processing\">Edge Data Processing</h2><ul>  <li>First, the predicted values are obtained using data up to just before the edge. Then, a spline fitting is applied to predict and generate edge data.</li></ul><h2 id=\"model-selection\">Model Selection</h2><h3 id=\"experimental-results-model-and-feature-wise-performance-comparison\">Experimental results (model and feature-wise performance comparison)</h3><hr /><p align=\"center\">    <img width=\"600\" src=\"/assets/2017/sige_thickness/All.png\" /></p><p align=\"center\">    <img width=\"600\" src=\"/assets/2017/sige_thickness/performance_compare.png\" /></p><h1 id=\"achievements\">Achievements</h1><hr /><ul>  <li>Cost efficiency achieved at 32% compared to competitor’s product.</li>  <li>Inspection speed achieved at 3 times faster compared to competitor’s product.</li>  <li>Performance reached 87% of competitor’s product.</li></ul><p><br /></p><h1 id=\"system-architecture\">System Architecture</h1><hr /><p align=\"center\">    <img width=\"600\" src=\"/assets/2017/sige_thickness/IPS_Model1.png\" /></p><p><br /></p><h1 id=\"final-result\">Final Result</h1><hr /><p align=\"center\">    <img width=\"600\" src=\"/assets/2017/sige_thickness/Analysis.png\" /></p><p align=\"center\">    <img width=\"600\" src=\"/assets/2017/sige_thickness/unisense_main.png\" /></p>",
            "url": "http://0.0.0.0:8700/2017/10/01/si-ge-thickness-and-composition-predict-system",
            
            
            
            
            
            "date_published": "2017-10-01T00:00:00+09:00",
            "date_modified": "2017-10-01T00:00:00+09:00",
            
                "author": 
                "{"twitter"=>nil, "name"=>nil, "avatar"=>nil, "email"=>nil, "url"=>nil}"
                
            
        }
    
    ]
}